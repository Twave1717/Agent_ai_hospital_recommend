# app.py

import streamlit as st
import os
import sys
from dotenv import load_dotenv
from google import genai
from google.genai import types

# .env íŒŒì¼ì—ì„œ í™˜ê²½ ë³€ìˆ˜ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.
load_dotenv()

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ë””ë ‰í„°ë¦¬ë¥¼ Python ê²½ë¡œì— ì¶”ê°€í•˜ì—¬ 'source' ëª¨ë“ˆì„ ì°¾ì„ ìˆ˜ ìˆë„ë¡ ì„¤ì •
sys.path.append(os.getcwd())

# ê¸°ì¡´ ì±—ë´‡ ë¡œì§ ë° ì„¤ì •ë“¤ì„ ì„í¬íŠ¸í•©ë‹ˆë‹¤.
from source.agent_veteran_skin_consultant import (
    upload_all_pdfs_once,
    get_pdf_summaries,
    create_pdf_selection_chain,
    create_category_extraction_chain,
    load_prompt_from_file,
    load_and_filter_hospitals,
    PROCEDURE_CATEGORIES,
    PROMPT_FILE_PATH,
    TEXTBOOK_DIR_PATH,
    HOSPITAL_CSV_PATH
)
from langchain_google_genai import ChatGoogleGenerativeAI

# --- Streamlit ì•± ì„¤ì • ---
st.set_page_config(page_title="AI í”¼ë¶€ê³¼ ìƒë‹´ ì±—ë´‡", page_icon="ğŸ‘©â€âš•ï¸")
st.title("ğŸ‘©â€âš•ï¸ AI í”¼ë¶€ê³¼ ìƒë‹´ ì±—ë´‡")
st.markdown("í”¼ë¶€ ì‹œìˆ ì— ëŒ€í•´ ê¶ê¸ˆí•œ ì ì„ ë¬´ì—‡ì´ë“  ë¬¼ì–´ë³´ì„¸ìš”! AIê°€ ì „ë¬¸ ì„œì ì„ ì°¸ê³ í•˜ì—¬ ë‹µë³€í•´ ë“œë¦½ë‹ˆë‹¤.")

# --- 1. ì´ˆê¸°í™” ë° ë¦¬ì†ŒìŠ¤ ìºì‹± ---

@st.cache_resource
def initialize_chatbot():
    api_key = os.getenv("GOOGLE_API_KEY")
    if not api_key:
        st.error("`.env` íŒŒì¼ì— `GOOGLE_API_KEY='ë‹¹ì‹ ì˜ API í‚¤'` í˜•ì‹ìœ¼ë¡œ í‚¤ë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”.")
        st.stop()
    
    client = genai.Client(api_key=api_key)

    with st.spinner("ì „ë¬¸ ì„œì (PDF)ì„ ë¡œë”©í•˜ê³  ìˆìŠµë‹ˆë‹¤..."):
        pdf_handles_dict = upload_all_pdfs_once(TEXTBOOK_DIR_PATH, client)
        if not pdf_handles_dict:
            st.error("ì—…ë¡œë“œí•  PDF íŒŒì¼ì´ ì—†ì–´ ì±—ë´‡ì„ ì‹¤í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            st.stop()

    pdf_summaries_dict = get_pdf_summaries()
    llm_for_chains = ChatGoogleGenerativeAI(
        model="gemini-1.5-pro-latest",
        temperature=0,
        client=client
    )
    pdf_selector_chain = create_pdf_selection_chain(llm_for_chains, pdf_summaries_dict)
    category_extraction_chain = create_category_extraction_chain(llm_for_chains, PROCEDURE_CATEGORIES)
    system_prompt_template = load_prompt_from_file(PROMPT_FILE_PATH)

    return client, pdf_handles_dict, pdf_selector_chain, category_extraction_chain, system_prompt_template

try:
    g_client, g_pdf_handles, g_selector_chain, g_cat_chain, g_sys_prompt = initialize_chatbot()
except Exception as e:
    st.error(f"ì±—ë´‡ ì´ˆê¸°í™” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
    st.stop()

# --- 2. ì±„íŒ… ê¸°ë¡ ê´€ë¦¬ ---

if "messages" not in st.session_state:
    st.session_state.messages = []

for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# --- 3. ì‚¬ìš©ì ì…ë ¥ ë° ì±—ë´‡ ì‘ë‹µ ì²˜ë¦¬ ---

if prompt := st.chat_input("ì—¬ê¸°ì— ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”..."):
    with st.chat_message("user"):
        st.markdown(prompt)

    with st.chat_message("assistant"):
        with st.spinner("AI ìƒë‹´ ì‹¤ì¥ì´ ë‹µë³€ì„ ì¤€ë¹„ ì¤‘ì…ë‹ˆë‹¤..."):
            
            # ë‹¨ê³„ 1-3: ì •ë³´ ìˆ˜ì§‘ (PDF ì„ íƒ, ì¹´í…Œê³ ë¦¬ ì¶”ë¡ , ë³‘ì› ì •ë³´ í•„í„°ë§)
            selection_result = g_selector_chain.invoke({"query": prompt})
            selected_filename = selection_result.selected_filename
            selected_pdf_handle = g_pdf_handles.get(selected_filename)
            if selected_pdf_handle:
                st.info(f"ğŸ“š ì°¸ê³ ìë£Œ: `{selected_filename}`")

            category_info = g_cat_chain.invoke({"query": prompt})
            category = category_info.category if category_info.is_detected else None
            if category:
                st.info(f"ğŸ’¬ ì¶”ë¡ ëœ ì¹´í…Œê³ ë¦¬: `{category}`")

            hospital_info_str = load_and_filter_hospitals(HOSPITAL_CSV_PATH, category)



            try:
                # 1. APIì— ì „ë‹¬í•  'parts' ë¦¬ìŠ¤íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
                current_user_parts = [prompt]
                
                # 2. PDF íŒŒì¼ í•¸ë“¤(ë¦¬ëª¨ì»¨)ì´ ìˆìœ¼ë©´ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€í•©ë‹ˆë‹¤.
                if selected_pdf_handle:
                    current_user_parts.append(selected_pdf_handle)
                st.session_state.messages.append({"role": "user", "content": current_user_parts})
                
                # ë‹¨ê³„ 4: APIì— ì „ë‹¬í•  ì½˜í…ì¸  êµ¬ì„±
                final_prompt_text = g_sys_prompt.replace("((HOSPITAL_LIST))", hospital_info_str) \
                .replace("((SUBMITTED_PHOTOS))", "ì‚¬ìš©ìê°€ ì œì¶œí•œ ì´ë¯¸ì§€ê°€ ì—†ìŠµë‹ˆë‹¤.") \
                .replace("((CONVERSATION_HISTORY))", str(st.session_state.messages))

                # 3. [í…ìŠ¤íŠ¸, íŒŒì¼] í˜•íƒœì˜ ë¦¬ìŠ¤íŠ¸ë¥¼ contentsì— ê·¸ëŒ€ë¡œ ì „ë‹¬í•˜ì—¬ APIë¥¼ í˜¸ì¶œí•©ë‹ˆë‹¤.
                #    ì´ê²ƒì´ í…ìŠ¤íŠ¸ì™€ íŒŒì¼ì„ í•¨ê»˜ ì²˜ë¦¬í•˜ëŠ” ì˜¬ë°”ë¥¸ ë°©ë²•ì…ë‹ˆë‹¤.
                response = g_client.models.generate_content(
                    model="gemini-1.5-flash-latest",
                    contents=final_prompt_text,
                    config=types.GenerateContentConfig(temperature=0.3)
                )
                response_text = response.text

            except Exception as e:
                response_text = f"ì£„ì†¡í•©ë‹ˆë‹¤, ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}"
                st.error(response_text)

        st.markdown(response_text)
        st.session_state.messages.append({"role": "assistant", "content": response_text})