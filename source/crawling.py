import time
import pandas as pd
import multiprocessing
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from selenium.webdriver.common.keys import Keys # Keys ì„í¬íŠ¸
from webdriver_manager.chrome import ChromeDriverManager
from bs4 import BeautifulSoup
from selenium_stealth import stealth
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, ElementNotInteractableException

# í¬ë¡¤ë§í•  ì¹´í…Œê³ ë¦¬ì™€ URL ëª©ë¡
URLS = {
    "ë¦¬í”„íŒ…": "https://www.gangnamunni.com/events?q=%EB%A6%AC%ED%94%84%ED%8C%85",
    "í”¼ë¶€": "https://www.gangnamunni.com/events?q=%ED%94%BC%EB%B6%80",
    "ì§€ë°©ì„±í˜•": "https://www.gangnamunni.com/events?q=%EC%A7%80%EB%B0%A9%EC%8%B1%ED%98%95",
    "ë³´í†¡ìŠ¤": "https://www.gangnamunni.com/events?q=%EB%B3%B4%ED%86%A1%EC%8A%A4",
    "í•„ëŸ¬": "https://www.gangnamunni.com/events?q=%ED%95%84%EB%9F%AC",
    "ì œëª¨": "https://www.gangnamunni.com/events?q=%EC%A0%9C%EB%AA%A8",
    "ëª¨ë°œì´ì‹": "https://www.gangnamunni.com/events?q=%EB%AA%A8%EB%B0%9C%EC%9D%B4%EC%8B%9D"
}

OUTPUT_FILE = 'gangnam_unni_final_aggressive.csv'

def scrape_category(category, url, output_file, lock):
    print(f"âœ… [{category}] ì‘ì—… ì‹œì‘...")
    
    driver = None
    try:
        service = Service(ChromeDriverManager().install())
        options = webdriver.ChromeOptions()
        options.add_argument('--headless')
        options.add_argument('--no-sandbox')
        options.add_argument('--disable-dev-shm-usage')
        options.add_argument('--window-size=1920x1080')
        options.add_argument("user-agent=Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 Safari/537.36")
        
        driver = webdriver.Chrome(service=service, options=options)
        stealth(driver, languages=["ko-KR", "ko"], vendor="Google Inc.", platform="MacIntel",
                webgl_vendor="Intel Inc.", renderer="Intel Iris OpenGL Engine", fix_hairline=True)
        
        driver.get(url)
        
        try:
            wait = WebDriverWait(driver, 15)
            scroll_container = wait.until(EC.visibility_of_element_located((By.ID, "frameMain")))
        except TimeoutException:
            print(f"âŒ [{category}] ìŠ¤í¬ë¡¤ ì»¨í…Œì´ë„ˆë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
            return

        scraped_links = set()
        all_events_data = []
        patience = 10 # ì¸ë‚´ì‹¬ íšŸìˆ˜ë¥¼ 10ìœ¼ë¡œ ëŠ˜ë¦¼

        print(f"ğŸ“œ [{category}] ìµœì¢… ìŠ¤í¬ë¡¤ ë¡œì§ ì‹œì‘...")
        while patience > 0:
            previous_item_count = len(scraped_links)

            # --- 1. ìë°”ìŠ¤í¬ë¦½íŠ¸ë¡œ ìŠ¤í¬ë¡¤ ---
            driver.execute_script("arguments[0].scrollTop = arguments[0].scrollHeight", scroll_container)
            
            # --- 2. í‚¤ë³´ë“œ Page Down í‚¤ ì „ì†¡ìœ¼ë¡œ ì¶”ê°€ ìŠ¤í¬ë¡¤ ì‹œë„ ---
            body = driver.find_element(By.TAG_NAME, 'body')
            body.send_keys(Keys.PAGE_DOWN)
            time.sleep(2.5)
            
            # í˜„ì¬ í™”ë©´ì˜ ë°ì´í„° íŒŒì‹±
            soup = BeautifulSoup(driver.page_source, 'html.parser')
            events = soup.select('a[id^="event-card-component-ui-"]')
            
            for event in events:
                link = event.get('href')
                if link and link not in scraped_links:
                    scraped_links.add(link)
                    
                    try:
                        title = event.select_one('h2').get_text(strip=True)
                        clinic_info = event.select_one('h2 + span').get_text(strip=True)
                        price = event.select_one('h3').get_text(strip=True)
                        location, hospital_name = clinic_info.split('ãƒ»') if 'ãƒ»' in clinic_info else ('N/A', clinic_info)

                        all_events_data.append({
                            'ì¹´í…Œê³ ë¦¬': category, 'ë³‘ì› ì´ë¦„': hospital_name.strip(), 'ìœ„ì¹˜': location.strip(),
                            'ì´ë²¤íŠ¸ ì œëª©': title, 'ê°€ê²©': price
                        })
                    except (AttributeError, IndexError):
                        continue
            
            # --- 3. ì‹¤ì œ ë°ì´í„° ì¦ê°€ ì—¬ë¶€ë¡œ ì¢…ë£Œ ì¡°ê±´ íŒë‹¨ ---
            current_item_count = len(scraped_links)
            if current_item_count == previous_item_count:
                patience -= 1
                print(f"âš ï¸ [{category}] ìƒˆë¡œìš´ ë°ì´í„° ì—†ìŒ. (ë‚¨ì€ ê¸°íšŒ: {patience})")
                
                # --- 4. 'ë”ë³´ê¸°' ë²„íŠ¼ì´ ìˆëŠ”ì§€ í™•ì¸í•˜ê³  í´ë¦­ ì‹œë„ ---
                try:
                    more_button = driver.find_element(By.XPATH, "//button[contains(text(), 'ë”ë³´ê¸°')]")
                    if more_button.is_displayed():
                        print("ğŸ–±ï¸ 'ë”ë³´ê¸°' ë²„íŠ¼ ë°œê²¬! í´ë¦­í•©ë‹ˆë‹¤.")
                        more_button.click()
                        patience = 10 # ë²„íŠ¼ í´ë¦­ í›„ ë‹¤ì‹œ ì¸ë‚´ì‹¬ ì´ˆê¸°í™”
                except (ElementNotInteractableException, Exception):
                    pass # ë²„íŠ¼ì´ ì—†ê±°ë‚˜ í´ë¦­í•  ìˆ˜ ì—†ìœ¼ë©´ ê·¸ëƒ¥ ë„˜ì–´ê°

            else:
                patience = 10 # ìƒˆë¡œìš´ ë°ì´í„°ê°€ ìˆìœ¼ë©´ ì¸ë‚´ì‹¬ ì´ˆê¸°í™”
                print(f"ğŸ“„ [{category}] {current_item_count}ê°œ ìˆ˜ì§‘ ì™„ë£Œ...")

        if all_events_data:
            df = pd.DataFrame(all_events_data)
            with lock:
                df.to_csv(output_file, mode='a', header=False, index=False, encoding='utf-8-sig')
            print(f"ğŸ‘ [{category}] ì‘ì—… ì™„ë£Œ! ì´ {len(all_events_data)}ê°œ ì‹ ê·œ ì €ì¥.")
        else:
            print(f"âš ï¸ [{category}] ìˆ˜ì§‘ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

    except Exception as e:
        print(f"âŒ [{category}] ì‘ì—… ì¤‘ ì¹˜ëª…ì  ì˜¤ë¥˜ ë°œìƒ: {e}")
    finally:
        if driver:
            driver.quit()

# ë©”ì¸ ì‹¤í–‰ ë¶€ë¶„ (ì´ì „ê³¼ ë™ì¼)
if __name__ == "__main__":
    core_count = multiprocessing.cpu_count()
    PROCESSES = min(4, core_count)
    print(f"ë³‘ë ¬ ì‘ì—…ì„ ìœ„í•´ {PROCESSES}ê°œì˜ í”„ë¡œì„¸ìŠ¤ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
    
    df_header = pd.DataFrame(columns=['ì¹´í…Œê³ ë¦¬', 'ë³‘ì› ì´ë¦„', 'ìœ„ì¹˜', 'ì´ë²¤íŠ¸ ì œëª©', 'ê°€ê²©'])
    df_header.to_csv(OUTPUT_FILE, index=False, encoding='utf-8-sig')

    manager = multiprocessing.Manager()
    lock = manager.Lock()
    tasks = [(category, url, OUTPUT_FILE, lock) for category, url in URLS.items()]
    
    with multiprocessing.Pool(processes=PROCESSES) as pool:
        pool.starmap(scrape_category, tasks)

    print(f"\nğŸ‰ ëª¨ë“  ë³‘ë ¬ í¬ë¡¤ë§ ì‘ì—…ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤! '{OUTPUT_FILE}' íŒŒì¼ì„ í™•ì¸í•˜ì„¸ìš”.")